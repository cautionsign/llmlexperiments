This is a phrase from La Monte Young that impressed me tremendously. For 25 years, I have been inspired by it.

![](https://miro.medium.com/v2/resize:fit:1400/1*KuUyaXCjl4CF4ov9dzylXQ.png)

> Make a visual and poetic piece with Python about what happens when we die

This was the result of a poem turned into ASCII art

The ASCII art is beautiful, like clouds or waveforms.

I am beginning to understand how prompts exist in history — within the context window, as it is called. The Hegelian theory of prompts rather than the Kantian theory of prompts. The historical process of prompts.

I was at [CCAM](https://yalemachine.me/) the other day — which was terrific. I talked about AI and the Tarot. I say AI (use) is like tarot — symbolic — the meaning is language is created from cultural history. Someone said ai (technology/word embeddings) is like music — indexical/psychological / CS Peirce — the meaning is created via transference by the subject/the querant.

In Jungian alchemical relationships — relationships have their own lifeforce — their own being. The coming together of two transferences the creation of a third thing. Can this happen with an AI? Can an AI have transference or counter-transference? No. The AI has no symbolic meaning, only indexical meaning.

There is a new question now beyond Shannon — beyond information. What makes one thing more meaningful than another is meaningfulness, always subjective, always a product of transference. Is meaning anthropocentric? If information is about goal following, meaning is about emotion.

This is the affect model —

There are three kinds of models: concrete (a to-scale replica), mathematical (an equation F=MA), and simulation (run through all possible options or scenarios). What is a neural network? It is none of these things. What is Tarot? It is none of these things. Trained by experience, we judge the model's effectiveness by our feelings about the response. The weights of the model and the intuition of the reader are emotional responses. An emotion is subjective; it becomes objective via embodied actions in the world, not language. The weights of a neural network are embodied.

Thank you for reading these disorganized thoughts.

[The latest code](https://github.com/msrobot0/llmlexperiments/).